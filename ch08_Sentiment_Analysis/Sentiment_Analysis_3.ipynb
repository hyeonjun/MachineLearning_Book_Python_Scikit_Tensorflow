{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch08_Sentiment_Analysis_3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNmRBzAowOibCctTviwN5OC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 8.4 대용량 데이터 처리: 온라인 알고리즘과 외부 메모리 학습\n",
        "* 외부 메모리 학습\n",
        "    * 데이터셋을 작은 배치(batch) 단위로 나누어 분류기를 점진적으로 학습\n",
        "    * CountVectorizer 클래스 사용 불가.\n",
        "        * 전체 어휘 사전을 메모리에 가지고 있어야 하기 때문.\n",
        "    * TfidfVectorizer 클래스 사용 불가.\n",
        "        * 역문서 빈도를 계산하기 위해 훈련 데이터셋의 특성 벡터를 모두 메모리에 가지고 있어야 하기 때문\n",
        "    * 사이킷런 HashingVectorizer 클래스\n",
        "        * 데이터 종류에 상관없이 사용 가능.\n",
        "        * 32비트 MurmurHash3 함수를 사용한 해싱 트릭 사용."
      ],
      "metadata": {
        "id": "IzfuGeKvg-IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwKInXzvlAJj",
        "outputId": "c5cf04c8-7e55-49f0-97c1-f3fa6a4b8621"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WIkks2aHegaf"
      },
      "outputs": [],
      "source": [
        "# tokenizer() : 텍스트 데이터를 정제하고 불용어를 제외한 후 단어 토큰으로 분리\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
        "    tokenized = [w for w in text.split() if w not in stop]\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "# stream_docs() : 한 번에 한 문서씩 읽어서 반환하는 제너레이터\n",
        "def stream_docs(path):\n",
        "    with open(path, 'r', encoding='utf-8') as csv:\n",
        "        next(csv)  # 헤더 넘기기\n",
        "        for line in csv:\n",
        "            text, label = line[:-3], int(line[-2])\n",
        "            yield text, label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# movie_data.csv 파일에서 첫번재 문서를 읽는다\n",
        "next(stream_docs(path='/content/drive/MyDrive/머신러닝 교과서 with 파이썬, 사이킷런, 텐서플로/data/movie_data.csv'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQJHKnTBlEKW",
        "outputId": "4ebaa337-8063-4e81-9cb4-9eee83e991dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
              " 1)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get_minibatch() : stream_docs 함수에서 문서를 읽어 size 매개변수에서 지정한 만큼 문서를 반환\n",
        "def get_minibatch(doc_stream, size):\n",
        "    docs, y = [], []\n",
        "    try:\n",
        "        for _ in range(size):\n",
        "            text, label = next(doc_stream)\n",
        "            docs.append(text)\n",
        "            y.append(label)\n",
        "    except StopIteration:\n",
        "        return None, None\n",
        "    return docs, y"
      ],
      "metadata": {
        "id": "8NcgHX2pltGj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "vect = HashingVectorizer(decode_error='ignore', \n",
        "                         n_features=2**21, # 특성 개수 -> 크게하면 해시 충돌 가능성을 줄일 수 있지만, 로지스틱 회귀 모델의 가중치 개수도 늘어난다.\n",
        "                         preprocessor=None, \n",
        "                         tokenizer=tokenizer)\n",
        "\n",
        "clf = SGDClassifier(loss='log', random_state=1)\n",
        "\n",
        "doc_stream = stream_docs(path='/content/drive/MyDrive/머신러닝 교과서 with 파이썬, 사이킷런, 텐서플로/data/movie_data.csv')"
      ],
      "metadata": {
        "id": "5UeztaoWmord"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyprind"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "043R5lw4n-ZV",
        "outputId": "e569e982-ce0b-4202-9691-4e4b4b6e1a81"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyprind in /usr/local/lib/python3.7/dist-packages (2.11.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 외부 메모리 학습\n",
        "import pyprind\n",
        "pbar = pyprind.ProgBar(45)\n",
        "\n",
        "classes = np.array([0, 1])\n",
        "for _ in range(45):\n",
        "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
        "    if not X_train:\n",
        "        break\n",
        "    X_train = vect.transform(X_train)\n",
        "    clf.partial_fit(X_train, y_train, classes=classes)\n",
        "    pbar.update()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gYEcWlunbPj",
        "outputId": "2fb431b0-b2e5-4d80-d61a-b7939aa507e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "진행 막대(PyPrind.ProgBar) 객체를 45번 반복으로 초기화하고 이어지는 fot 반복문에서 45개의 미니 배치를 반복한다. 각 미니배치를 1,000개의 문서로 구성된다. "
      ],
      "metadata": {
        "id": "UQjJoBkPoq06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 점진적인 학습 과정이 끝나면 마지막 5,000개의 문서를 사용하여 모델 성능 평가\n",
        "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
        "X_test = vect.transform(X_test)\n",
        "print('정확도: %.3f' % clf.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMni7DIso83i",
        "outputId": "618eafb2-80e8-4161-89ae-4e037fb38504"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정확도: 0.868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 외부 메모리 학습\n",
        "    * 메모리 효율성이 좋고, 모델 훈련 속도가 빠르다."
      ],
      "metadata": {
        "id": "5NQKZPpQpO6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 나머지 5,000개의 문서를 사용하여 모델 업데이트\n",
        "clf = clf.partial_fit(X_test, y_test)"
      ],
      "metadata": {
        "id": "It4mtNzVpYB5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 참고\n",
        "    * word2vec 모델\n",
        "        * 신경망을 기반으로 한 비지도 학습 알고리즘으로 자동으로 단어 사이의 관계를 학습.\n",
        "        * 비슷한 의미를 가진 단어를 비슷한 클러스터로 모으는 것.\n",
        "        * word2vec에서 만든 벡터 공간을 사용하면 모델이 간단한 벡터 연산으로 단어를 생성할 수 있다.\n",
        "        * https://code.google.com/p/word2vec/"
      ],
      "metadata": {
        "id": "df4m_0vmpl1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.5 잠재 디리클레 할당을 사용한 토픽 모델링\n",
        "* 토픽 모델링(topic modeling)\n",
        "    * 레이블이 없는 텍스트 문서에 토픽을 할당하는 광범위한 분야\n",
        "    * 카테고리 레이블을 할당\n",
        "    * 비지도 학습의 클러스터링\n",
        "    * 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)\n",
        "\n",
        "### 8.5.1 LDA를 사용한 텍스트 문서 분해\n",
        "* 여러 문서에 걸쳐 자주 등장하는 단어의 그룹을 찾는 확률적 생성 모델\n",
        "    * LDA는 Count기반의 Vectorizer만 적용.\n",
        "    * LDA는 입력 : BoW 모델\n",
        "        * 입력으로 받은 BoW 모델을 두 개의 행렬로 분해\n",
        "            * 문서-토픽 행렬\n",
        "            * 단어-토픽 행렬\n",
        "    * 분해한 두 행렬을 곱해서 가능한 작은 오차로 BoW 입력 행렬을 재구성할 수 있도록 LDA가 BoW 행렬을 분해한다.\n",
        "    * 단점 : 미리 토픽 개수를 정해야한다.\n",
        "        * 토픽 개수는 LDA의 하이퍼 파라미터로, 수동으로 지정.\n",
        "\n",
        "### 8.5.2 사이킷런 LDA\n",
        "* 사이킷런 LatentDirichletAllocation 클래스"
      ],
      "metadata": {
        "id": "0_6cbtQnpi96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/머신러닝 교과서 with 파이썬, 사이킷런, 텐서플로/data/movie_data.csv', encoding='utf-8')\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "TZjx_vQ9tBLb",
        "outputId": "63909434-66e6-4a84-8ea8-4117625cfe1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5bc39269-fc58-4b9c-8859-adcab0b3fa86\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5bc39269-fc58-4b9c-8859-adcab0b3fa86')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5bc39269-fc58-4b9c-8859-adcab0b3fa86 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5bc39269-fc58-4b9c-8859-adcab0b3fa86');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer(stop_words='english',\n",
        "                        max_df=.1, # 최대 문서 빈도 10%로 지정하여 여러 문서에 걸쳐 너무 자주 등장하는 단어를 제외\n",
        "                        max_features=5000) # 가장 자주 등장하는 단어 5,000개로 단어 수를 제한.\n",
        "X = count.fit_transform(df['review'].values)"
      ],
      "metadata": {
        "id": "fTUZtcY1tMK6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 자주 등장하는 단어를 제외하는 이유\n",
        "    * 모든 문서에 걸쳐 등장하는 단어일 수 있다.\n",
        "    * 이러한 단어는 문서의 특정 토픽 카테고리와 관련성이 적다.\n",
        "* 자주 등장하는 단어 수 제한하는 이유\n",
        "    * 데이터셋의 차원을 제한하여 LDA 추론 성능을 향상시킨다."
      ],
      "metadata": {
        "id": "ErJ0gM9GtpBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=10,\n",
        "                                random_state=123,\n",
        "                                learning_method='batch') # 'batch' : lda 추정기가 한 번 반복할 때 가능한 모든 훈련 데이터(BoW)를 사용하여 학습된다.\n",
        "                                                         # 'online' 보다 느리지만 더 정확한 결과를 만든다.\n",
        "X_topics = lda.fit_transform(X)"
      ],
      "metadata": {
        "id": "WYQfIC8ruH0p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda.components_.shape # 열 개의 토픽에 대해 오름차순으로 단어의 중요도를 담은 행렬이 저장된다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B51Piu_euULp",
        "outputId": "ea430d0e-3b05-4b6e-c704-37158b33fcd4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 5000)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최상위 5개 단어 출력\n",
        "n_top_words = 5\n",
        "feature_names = count.get_feature_names_out()\n",
        "\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    print(\"Topic %d:\" % (topic_idx + 1))\n",
        "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1] ]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwBZDI4SyXVQ",
        "outputId": "e43ad227-ffb0-4408-dfb1-79ff3e3da4fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1:\n",
            "worst minutes awful script stupid\n",
            "Topic 2:\n",
            "family mother father children girl\n",
            "Topic 3:\n",
            "american war dvd music tv\n",
            "Topic 4:\n",
            "human audience cinema art sense\n",
            "Topic 5:\n",
            "police guy car dead murder\n",
            "Topic 6:\n",
            "horror house sex girl woman\n",
            "Topic 7:\n",
            "role performance comedy actor performances\n",
            "Topic 8:\n",
            "series episode war episodes tv\n",
            "Topic 9:\n",
            "book version original read novel\n",
            "Topic 10:\n",
            "action fight guy guys cool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 토픽에서 가장 중요한 단어 다섯 개를 기반으로 LDA가 다음 토픽을 구별했다고 추측할 수 있다.\n",
        "\n",
        "1.\t대체적으로 형편없는 영화(실제 토픽 카테고리가 되지 못함)\n",
        "2.\t가족 영화\n",
        "3.\t전쟁 영화\n",
        "4.\t예술 영화\n",
        "5.\t범죄 영화\n",
        "6.\t공포 영화\n",
        "7.\t코미디 영화\n",
        "8.\tTV 쇼와 관련된 영화\n",
        "9.\t소설을 원작으로 한 영화\n",
        "10.\t액션 영화"
      ],
      "metadata": {
        "id": "xIi32L4Ry6Xb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "카테고리가 잘 선택됐는지 확인하기 위해 공포 영화 카테고리에서 3개 영화의 리뷰를 출력(공포 영화는 카테고리 6, 인덱스로는 5)"
      ],
      "metadata": {
        "id": "RVC930QCzA5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "horror = X_topics[:, 5].argsort()[::-1]\n",
        "\n",
        "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
        "    print('\\n공포 영화 #%d:' % (iter_idx + 1))\n",
        "    print(df['review'][movie_idx][:300], '...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2zNj1EEzAHg",
        "outputId": "818898f0-1b46-4761-8fe8-d987e27d72d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "공포 영화 #1:\n",
            "House of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n",
            "\n",
            "공포 영화 #2:\n",
            "Okay, what the hell kind of TRASH have I been watching now? \"The Witches' Mountain\" has got to be one of the most incoherent and insane Spanish exploitation flicks ever and yet, at the same time, it's also strangely compelling. There's absolutely nothing that makes sense here and I even doubt there  ...\n",
            "\n",
            "공포 영화 #3:\n",
            "<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ...\n"
          ]
        }
      ]
    }
  ]
}