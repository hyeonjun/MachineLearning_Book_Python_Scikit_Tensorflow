{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch12_Multi_layer_Neural_Network_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxA6RIjMu8YRgIByZRZtIO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5TAoHV2HN4EG"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.3 인공 신경망 훈련\n",
        "\n",
        "### 12.3.1 로지스틱 비용 함수 계산\n",
        "* 3차원 텐서 $W$\n",
        "    * 하나의 은닉층을 가진 다층 퍼셉트론\n",
        "        * $W^{(h)}$: 입력층과 은닉층을 연결하는 가중치 행렬\n",
        "        * $W^{(out)}$: 은닉층과 출력층을 연결하는 가중치 행렬\n",
        "        "
      ],
      "metadata": {
        "id": "jZDwt4ZfOHH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3차원 텐서\n",
        "Image(url='https://git.io/JLdov', width=300) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "qVJnMODoPMfH",
        "outputId": "7bb0a399-968c-4bbf-fea7-8108cd02a167"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://git.io/JLdov\" width=\"300\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP의 은닉 유닛, 출력 유닛, 입력 특성의 개수가 같지 않으면 $W^{(h)}$와 $W^{(out)}$은 같은 행과 열을 가지고 있지 않다."
      ],
      "metadata": {
        "id": "_HsrMxUCPWBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.3.2 역전파 알고리즘의 이해\n",
        "* 고차원 비용 함수의 곡면에는 전역 최솟값을 찾기 위해 넘어야할 지역 최솟값이 많기 때문에 학습이 어렵다.\n",
        "    * 다층 신경망에서 복잡한 비용 함수의 편미분을 효율적으로 계산하기 위한 방법\n",
        "        * 편미분을 사용하여 다층 인공 신경망의 가중치 파라미터를 학습\n",
        "* 자동 미분: 중첩된 함수의 도함수를 계산하는 방법\n",
        "    * 정방향\n",
        "        * 각 층마다 큰 행렬을 곱한 후 마지막에 벡터를 곱해 출력하는 방식\n",
        "    * 역방향\n",
        "        * 역전파는 역방향 자동 미분의 특수한 경우이다.\n",
        "        * 행렬과 벡터를 곱해 또 다른 벡터를 얻은 후 다음 행렬을 곱하는 방식\n",
        "    * 행렬-벡터 곱셈은 행렬-행렬 곱셈보다 훨씬 적은 계산 비용이 든다.\n",
        "\n",
        "### 12.3.3 역전파 알고리즘으로 신경망 훈련\n",
        "\n",
        "$Z^{(h)} = A^{(in)}W^{(h)}$ (은닉층의 최종 입력)\n",
        "\n",
        "$A^{(h)} = \\phi(Z^{(h)})$ (은닉층의 활성화 출력)\n",
        "\n",
        "$Z^{(out)} = A^{(h)}W^{(out)}$ (출력층의 최종 입력)\n",
        "\n",
        "$A^{(out)} = \\phi(Z^{(out)}$ (출력층의 활성화)\n"
      ],
      "metadata": {
        "id": "qt-rLPdyP7pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망의 정방향 계산\n",
        "# 입력 특성을 연결된 네트워크를 통해 앞으로 전파\n",
        "Image(url='https://git.io/JLdoa', width=400)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "i5w5X8XeSMn3",
        "outputId": "92dabb46-1fec-4ee1-8be3-46c6a1531688"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://git.io/JLdoa\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 역전파: 오차를 오른쪽에서 왼쪽으로 전파"
      ],
      "metadata": {
        "id": "73a2Mp7kSyRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 역전파 알고리즘\n",
        "Image(url='https://git.io/JLdoz', width=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "4T78C_8ectGd",
        "outputId": "eadcd892-72b6-48d9-e2f8-423abd55c48a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://git.io/JLdoz\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.4 신경망의 수렴\n",
        "온라인 학습에서는 한 번에 하나의 훈련 샘플(k=1)에 대한 그레이디언트를 계산하여 가중치를 업데이트 했다. 확률적이지만 매우 정확한 솔루션을 만들고 기본 경사 하강법보다 훨씬 빠르게 수렴한다. \n",
        "\n",
        "미니 배치 학습은 확률적 경사 하강법의 특별한 경우이다. n개의 훈련 샘플 중 k개의 부분 집합에서 그레이디언트를 계산한다(1<k<n). 미니 배치 학습은 벡터화된 구현을 만들어 계산 효율성을 놓일 수 있다는 것이 온라인 학습보다 장점이며, 기본 경사 하강법보다 훨씬 빠르게 가중치가 업데이트된다.\n",
        "\n",
        "다층 신경만은 일반적으로 최적화해야 할 가중치가 너무 많다. 그래서 손실 함수의 표면은 거칠어서 최적화 알고리즘이 쉽게 지역 최솟값에 갇힐 수 있다."
      ],
      "metadata": {
        "id": "d5SIyz9gc9P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 지역 최솟값과 전역 최솟값\n",
        "Image(url='https://git.io/JLdoK', width=500) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "tNyMbVdOeHCW",
        "outputId": "94937556-6800-4456-c295-6737a7f04ba3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://git.io/JLdoK\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습률을 크게 하면 지역 최솟값을 쉽게 탈출할 수 있지만, 너무 크면 전역 최솟값을 지나칠 수 있을 가능성이 높다."
      ],
      "metadata": {
        "id": "23PZlv0ceQhF"
      }
    }
  ]
}