{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch16_Language_Modeling_and_Transformer_Model_.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNmqUcmgtWJCYp8k33k8eaw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZGx_pkQoWIgL"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 글자 단위 언어 모델 구현\n",
        "* 언어 모델링: 영어 문장 생성처럼 기계가 사람의 언어와 관련된 작업을 수행하도록 만드는 애플리케이션.\n",
        "* 모델의 입력 = 텍스트 문서\n",
        "    * 입력 문서와 비슷한 스타일로 새로운 텍스트를 생성하는 모델을 만드는 것\n",
        "    * ex) 책, 특정 프로그래밍 언어 등\n",
        "    * 글자의 시퀀스로 나뉘어 한 번에 한 글자씩 네트워크에 주입\n",
        "        * 네트워크는 지금까지 본 글자와 함께 새로운 글자를 처리하여 다음 글자를 예측\n"
      ],
      "metadata": {
        "id": "uyXk5LG7Whyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 글자 단위 언어 모델링\n",
        "# EOS: 시퀀스의 끝을 의미(end of sequence)\n",
        "Image(url='https://git.io/JLdVE', width=700)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "ZJ-S2elJXe9o",
        "outputId": "738c29ef-87b6-43e9-ffae-abeea98cc34c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://git.io/JLdVE\" width=\"700\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 데이터셋 전처리\n",
        "* 글자 수준의 언어 모델링을 위한 데이터 준비\n",
        "    * 구텐베르크 프로젝트 웹 사이트에서 입력 데이터를 구한다.\n",
        "    * 쥘 베튼이 1874년에 출간한 <신비의 섬> 책의 텍스트를 사용\n",
        "        * http://www.gutenberg.org/files/1268/1268-0.txt\n",
        "        * curl -O http://www.gutenberg.org/files/1268/1268-0.txt"
      ],
      "metadata": {
        "id": "tiF-FvHHXpob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/rickiepark/python-machine-learning-book-3rd-edition/master/ch16/1268-0.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68zk9cA_Xzrg",
        "outputId": "05e3bab5-b5e1-48b8-db5b-0feb0d905694"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-17 08:35:55--  https://raw.githubusercontent.com/rickiepark/python-machine-learning-book-3rd-edition/master/ch16/1268-0.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1171600 (1.1M) [text/plain]\n",
            "Saving to: ‘1268-0.txt’\n",
            "\n",
            "1268-0.txt          100%[===================>]   1.12M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-02-17 08:35:56 (22.4 MB/s) - ‘1268-0.txt’ saved [1171600/1171600]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 텍스트를 읽어 전처리\n",
        "with open(\"1268-0.txt\", 'r', encoding='UTF8') as fp:\n",
        "    text = fp.read()\n",
        "\n",
        "print(text[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwcukvS2YTh4",
        "outputId": "982cd8d1-0606-4710-c076-83318318f0fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg EBook of The Mysterious Isl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
        "end_indx = text.find('End of the Project Gutenberg')\n",
        "print(start_indx, end_indx)\n",
        "\n",
        "text = text[start_indx:end_indx]\n",
        "char_set = set(text)\n",
        "print('전체 길이: ', len(text))\n",
        "print('고유 문자 길이: ', len(char_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8cvsguCYkXh",
        "outputId": "cb448943-8a94-457e-f2ef-a9436ebdd8be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "567 1112917\n",
            "전체 길이:  1112350\n",
            "고유 문자 길이:  80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "대부분의 신경망 라이브러리와 RNN 구현은 문자열 형태의 입력 데이터를 다룰 수 없다. 그래서 텍스트 데이터를 숫자 형태로 매핑해야하기 때문에 파이썬 딕셔너리 char2int를 만들어 각 문자를 정수로 매핑한다. 또한, 모델의 출력 결과를 텍스트로 변환하는 역 매핑도 필요하다. 정수와 문자를 키와 값으로 연결한 딕셔너리로 역 매핑을 수행할 수 있지만 인덱스와 고유 문자를 매핑한 넘파이 배열을 사용하는 것이 훨씬 효율적이다."
      ],
      "metadata": {
        "id": "26CeSx7SZBJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자와 인덱스 매핑\n",
        "Image(url='https://git.io/JLdVz', width=700)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "tp_f-PS_Zeep",
        "outputId": "a8317327-5fad-426c-c239-762ea993cf39"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://git.io/JLdVz\" width=\"700\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int = {ch:i for i, ch in enumerate(chars_sorted)}\n",
        "char_array = np.array(chars_sorted)\n",
        "\n",
        "text_encoded = np.array(\n",
        "    [char2int[ch] for ch in text],\n",
        "    dtype=np.int32)\n",
        "\n",
        "print('인코딩된 텍스트 크기: ', text_encoded.shape)\n",
        "\n",
        "print(text[:15], '      == 인코딩 ==> ', text_encoded[:15])\n",
        "print(text_encoded[15:21], '  == 디코딩 ==>', ''.join(char_array[text_encoded[15:21]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6uEQgNDZiYB",
        "outputId": "a2c1bcee-c586-4bfb-e587-42cbc8c5fc0e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코딩된 텍스트 크기:  (1112350,)\n",
            "THE MYSTERIOUS        == 인코딩 ==>  [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
            "[33 43 36 25 38 28]   == 디코딩 ==> ISLAND\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "넘파이 배열 text_encoded는 텍스트에 있는 모든 문자에 대한 인코딩 값을 담고 있다. 이 배열을 사용하여 텐서플로 데이터셋을 만든다."
      ],
      "metadata": {
        "id": "58a0IZjHavVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "ds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)\n",
        "\n",
        "for ex in ds_text_encoded.take(5):\n",
        "    print('{} -> {}'.format(ex.numpy(), char_array[ex.numpy()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JezFXoEyauSh",
        "outputId": "195e416b-b3c3-4810-e1da-460be068681b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44 -> T\n",
            "32 -> H\n",
            "29 -> E\n",
            "1 ->  \n",
            "37 -> M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트에 나타난 순서대로 문자를 담기 위해 반복 가능한 Dataset 객체를 만들었다."
      ],
      "metadata": {
        "id": "K66AJWWobPbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 불완전한 문자 시퀀스 집합에 텍스트 생성의 작업으로 Targets을 예측\n",
        "Image(url='https://git.io/JLdVV', width=700)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "J9dXojZFbgyI",
        "outputId": "74d590dc-dcad-46ba-a62f-8d7577cd76b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://git.io/JLdVV\" width=\"700\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "불완전한 문자 시퀀스에 대해 다음 문자를 예측해야 한다. 80개의 고유 문자가 있으므로 이 문제는 다중 분류 작업이 된다."
      ],
      "metadata": {
        "id": "I2glqBPUb13B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 다중 분류 방식을 기반으로 길이 1인 시퀀스(한 글자)로 시작해서 새로운 텍스트를 반복하여 생성\n",
        "# 새로운 텍스트 생성\n",
        "Image(url='https://git.io/JLdVr', width=700)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "FD31hM-Jb_Wg",
        "outputId": "2d2bdb05-b3e5-4b50-96f3-e9800423c322"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://git.io/JLdVr\" width=\"700\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텐서플로로 텍스트 생성 모델을 구현하기 위해 시퀀스 길이를 40으로 잘라 입력 텐서 x가 40개의 토큰으로 구성되도록 한다. 실제로 시퀀스 길이는 생성된 텍스트 품질에 영향을 미친다. 긴 시퀀스가 더 의미 있는 문장르 만들 수 있다. 하지만 짧은 시퀀스일 경우 모델이 대부분의 문맥을 무시하고 개별 단어를 정확히 감지하는 데 초점을 맞출 수 있다.\n",
        "\n",
        "긴 시퀀스가 보통 더 의미 있는 문장을 만들지만 긴 시퀀스에서 RNN모델이 장기간 의존성을 감지하기 어렵다. 따라서 실제로 **적절한 시퀀스 길이를 찾는 것은 경험적으로 평가해야 하는 하이퍼파라미터 최적화 문제**이다.\n",
        "\n",
        "위 그림에서 보듯 입력 x와 타깃 y는 한 글자씩 어긋나 있다. 따라서 텍스트는 41 문자씩나눈다. 처음부터 40개의 문자는 입력 시퀀스 x가 되고 마지막 문자는 타깃 시퀀스 y가 된다.\n",
        "\n",
        "Dataset 객체 ds_text_encoded에 인코딩된 전체 텍스르르 원본 문자 순서대로 저장해놓았다. batch() 메서드를 사용하여 41개의 문자로 구성된 텍스트 조각을 만들고, 마지막 배치에서 길이가 41보다 작으면 이 배치는 버린다. 즉, 데이터셋은 항상 길이가 41인 시퀀스를 담고 있다. 이 41개의 문자 조각을 사용해서 시퀀스 x(입력)와 시퀀스 y(타깃)을 만든다. 두 시퀀스는 모두 40개의 원소로 구성된다."
      ],
      "metadata": {
        "id": "YJcHzisecPEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "\n",
        "ds_chunks = ds_text_encoded.batch(chunk_size, drop_remainder=True)\n",
        "\n",
        "# inspection\n",
        "for seq in ds_chunks.take(1):\n",
        "    input_seq = seq[:seq_length].numpy()\n",
        "    target = seq[seq_length].numpy()\n",
        "    print(input_seq, ' -> ', target)\n",
        "    print(repr(''.join(char_array[input_seq])), \n",
        "          ' -> ', repr(''.join(char_array[target])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4uwvmUBd9Ly",
        "outputId": "78ccf829-33f9-404e-a63c-da43c41b259e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[44 32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  1  6  6\n",
            "  6  0  0  0  0  0 40 67 64 53 70 52 54 53  1 51]  ->  74\n",
            "'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'  ->  'y'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x & y를 나누기 위한 함수\n",
        "def split_input_target(chunk):\n",
        "    input_seq = chunk[:-1] # 0 ~ 40\n",
        "    target_seq = chunk[1:] # 1 ~ 41\n",
        "    return input_seq, target_seq\n",
        "\n",
        "ds_sequences = ds_chunks.map(split_input_target)\n",
        "\n",
        "# 확인\n",
        "for ex in ds_sequences.take(2):\n",
        "    print('입력 (x):', repr(''.join(char_array[ex[0].numpy()])))\n",
        "    print('입력 (x):', repr(''.join(char_array[ex[1].numpy()])))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsysaXgEe25z",
        "outputId": "54c494a2-ce11-4af1-de80-bfe79cfa90fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\n",
            "입력 (x): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
            "\n",
            "입력 (x): ' Anthony Matonak, and Trevor Carlson\\n\\n\\n\\n'\n",
            "입력 (x): 'Anthony Matonak, and Trevor Carlson\\n\\n\\n\\n\\n'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋 준비의 마지막 단계로 이 데이터셋을 미니 배치로 나눈다. 데이터셋을 배치로 나누기 위해 첫 번째 전처리 단계에서 문장 조각을 만들었다. 각 조각이 하나의 훈련 샘플을 대응하는 문장을 표현한다."
      ],
      "metadata": {
        "id": "Smn2PwHRfjio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "ds = ds_sequences.shuffle(BUFFER_SIZE).batch(BATCH_SIZE) # drop_remainder=True)\n",
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87u-zFJIfiDR",
        "outputId": "d82dbea8-e467-4211-eb44-0583e789ec0a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None, 40), dtype=tf.int32, name=None), TensorSpec(shape=(None, 40), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 문자 수준의 RNN 모델"
      ],
      "metadata": {
        "id": "kAsARoHOgL1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 재사용을 위해 케라스 Sequential 클래스로 모델을 만드는 함수\n",
        "def build_model(vocab_size, embedding_dim, rnn_units):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "        tf.keras.layers.LSTM(\n",
        "            rnn_units, return_sequences=True),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "JiH0IVwrgOsh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "charset_size = len(char_array)\n",
        "embedding_dim = 256\n",
        "rnn_units=512\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "model = build_model(\n",
        "    charset_size,\n",
        "    embedding_dim,\n",
        "    rnn_units)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJRuROYZguDR",
        "outputId": "1eb562cc-c01d-47b6-9411-7f9916e43c89"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 256)         20480     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 512)         1574912   \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 80)          41040     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,636,432\n",
            "Trainable params: 1,636,432\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LSTM 층의 출력 크기 = (None, None, 512), 랭크 3\n",
        "    * 첫 번째 차원: 배치 차원\n",
        "    * 두 번째 차원: 출력 시퀀스 길이\n",
        "    * 마지막 차원: 은닉 유닛의 개수\n",
        "    * 랭크 3의 출력을 만드는 이유\n",
        "        * LSTM 층을 만들 때 return_sequences=True로 지정했기 때문이다.\n",
        "        * 완전 연결 층(Dense)이 LSTM 층의 출력을 받아 출력 시퀀스의 각 원소마다 로짓을 계산한다. 결국 이 모델의 최종 출력도 랭크 3 텐서가 된다.\n",
        "    \n",
        "* 마지막 완전 연결 층을 activation=None 으로 설정\n",
        "    * 새로운 텍스트를 생성하기 위해 모델 예측 값에서 샘플링할 수 있도록 로짓 출력이 필요하기 때문이다."
      ],
      "metadata": {
        "id": "G9-OD5zPhDDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True\n",
        "    ))\n",
        "model.fit(ds, epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORPyr-V1hB1K",
        "outputId": "82e1fb90-ee08-4dbf-dd61-1eec10b3a07f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "424/424 [==============================] - 28s 36ms/step - loss: 2.3212\n",
            "Epoch 2/20\n",
            "424/424 [==============================] - 16s 36ms/step - loss: 1.7581\n",
            "Epoch 3/20\n",
            "424/424 [==============================] - 16s 35ms/step - loss: 1.5524\n",
            "Epoch 4/20\n",
            "424/424 [==============================] - 16s 35ms/step - loss: 1.4330\n",
            "Epoch 5/20\n",
            "424/424 [==============================] - 16s 37ms/step - loss: 1.3577\n",
            "Epoch 6/20\n",
            "424/424 [==============================] - 16s 35ms/step - loss: 1.3056\n",
            "Epoch 7/20\n",
            "424/424 [==============================] - 16s 35ms/step - loss: 1.2656\n",
            "Epoch 8/20\n",
            "424/424 [==============================] - 16s 35ms/step - loss: 1.2340\n",
            "Epoch 9/20\n",
            "424/424 [==============================] - 16s 36ms/step - loss: 1.2086\n",
            "Epoch 10/20\n",
            "424/424 [==============================] - 16s 35ms/step - loss: 1.1860\n",
            "Epoch 11/20\n",
            "424/424 [==============================] - 16s 35ms/step - loss: 1.1665\n",
            "Epoch 12/20\n",
            "424/424 [==============================] - 19s 44ms/step - loss: 1.1487\n",
            "Epoch 13/20\n",
            "424/424 [==============================] - 16s 35ms/step - loss: 1.1318\n",
            "Epoch 14/20\n",
            "424/424 [==============================] - 16s 35ms/step - loss: 1.1166\n",
            "Epoch 15/20\n",
            "424/424 [==============================] - 16s 35ms/step - loss: 1.1022\n",
            "Epoch 16/20\n",
            "424/424 [==============================] - 16s 36ms/step - loss: 1.0890\n",
            "Epoch 17/20\n",
            "424/424 [==============================] - 16s 35ms/step - loss: 1.0753\n",
            "Epoch 18/20\n",
            "424/424 [==============================] - 16s 35ms/step - loss: 1.0622\n",
            "Epoch 19/20\n",
            "424/424 [==============================] - 16s 35ms/step - loss: 1.0498\n",
            "Epoch 20/20\n",
            "424/424 [==============================] - 16s 36ms/step - loss: 1.0368\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2c896e3e90>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 평가: 새로운 텍스트 생성\n",
        "\n",
        "훈련한 RNN 모델은 각 문자에 대해 80개 크기의 로짓을 반환한다. **소프트맥스 함수를 사용해 이 로짓을 쉽게 확률**로 바꿀 수 있다. 이 확률을 사용하여 다음 문자를 결정한다. 시퀀스에서 다음 문자를 예측하기 위해 간단히 가장 큰 로짓 값을 가진 원소를 선택할 수 있다. 하지만 항상 가장 높은 확률을 가진 문자를 선택하는 대신 **출력에서 (랜덤하게) 샘플링**하려고 한다. 이렇게 하지 않으면 모델이 항상 동일한 텍스트를 만든다.\n",
        "\n",
        "텐서플로의 tf.random.categorical() 함수를 사용하여 범주형 분포에서 랜덤하게 샘플링할 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "BMgnoXCEiD4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)\n",
        "\n",
        "logits = [[1.0, 1.0, 1.0]]\n",
        "print('확률: ', tf.math.softmax(logits).numpy()[0])\n",
        "\n",
        "samples = tf.random.categorical(\n",
        "    logits=logits, num_samples=10)\n",
        "tf.print(samples.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeFHjqRpiw_C",
        "outputId": "02a2aa59-71d9-4b87-8f2c-feb6244ee922"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확률:  [0.33333334 0.33333334 0.33333334]\n",
            "array([[1, 2, 0, 1, 0, 1, 1, 2, 1, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "로짓이 같아 이 범주는 동일한 확률을 가진다.(즉, 범위 선택 가능성이 동일) 따라서 샘플 크기가 크면 각 범주가 등장할 횟수는 샘플 크기의 1/3에 이를 것으로 기대할 수 있다. 로짓을 [1, 1, 3]으로 바꾸면 범주 2가 더 많이 등장할 것이다."
      ],
      "metadata": {
        "id": "ILGdtFQrjesp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)\n",
        "\n",
        "logits = [[1.0, 1.0, 3.0]]\n",
        "print('확률:', tf.math.softmax(logits).numpy()[0])\n",
        "\n",
        "samples = tf.random.categorical(\n",
        "    logits=logits, num_samples=10)\n",
        "tf.print(samples.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2sqwcqpj2Ez",
        "outputId": "78618be9-3879-4b86-dce1-c2cb2398d435"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확률: [0.10650698 0.10650698 0.78698605]\n",
            "array([[2, 2, 0, 2, 2, 2, 2, 2, 1, 2]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf.random.categorical 함수를 사용하면 모델이 추력한 로짓을 기반으로 문자를 생성할 수 있다. 짧은 시작 문자열 starting_str을 받아 새로운 generated_str을 생성하는 sample() 함수를 정의한다..\n",
        "\n"
      ],
      "metadata": {
        "id": "EtRfPN-YjZyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, starting_str, \n",
        "           len_generated_text=500, \n",
        "           max_input_length=40,\n",
        "           scale_factor=1.0):\n",
        "    encoded_input = [char2int[s] for s in starting_str]\n",
        "    encoded_input = tf.reshape(encoded_input, (1, -1)) # 정수 시퀀스\n",
        "\n",
        "    generated_str = starting_str # 초기의 입력 값으로 설정\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(len_generated_text):\n",
        "        # RNN 모델에 전달하여 로짓 계산, RNN 모델의 마지막 순환 층에서 \n",
        "        # return_sequences=True로 설정해 입력 시퀀스와 동일한 길이의 로짓 시퀀스가 출력된다.\n",
        "        # 따라서 RNN 모델의 출력에 있는 각 원소는 모델이 입력 시퀀스를 관찰해 다음 문자를 위한 로짓을 표현한다.(80개의 원소 벡터)\n",
        "        logits = model(encoded_input)\n",
        "        logits = tf.squeeze(logits, 0)\n",
        "\n",
        "        scaled_logits = logits * scale_factor\n",
        "        # 출력 logits의 마지막 원소만 tf.random.categorical() 함수로 전달하여 새로운 샘플 생성\n",
        "        new_char_indx = tf.random.categorical(scaled_logits, num_samples=1)\n",
        "\n",
        "        # 새로운 샘플을 문자로 변환하고 생성된 문자열 generated_text 끝에 추가하여 길이를 1만큼 늘린다.\n",
        "        new_char_indx = tf.squeeze(new_char_indx)[-1].numpy() \n",
        "        generated_str += str(char_array[new_char_indx])\n",
        "\n",
        "        new_char_indx = tf.expand_dims([new_char_indx], 0)\n",
        "        encoded_input = tf.concat(\n",
        "            [encoded_input, new_char_indx],\n",
        "            axis=1)\n",
        "        # 지정한 문자 길이만큼 생성될 때까지 generated_text에서 마지막 max_input_length 개의 문자를 선택하고\n",
        "        # 이를 사용해서 새로운 문자를 생성한다.\n",
        "        encoded_input = encoded_input[:, -max_input_length:]\n",
        "    \n",
        "    return generated_str\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "print(sample(model, starting_str='The island'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_Gqxcr_k__y",
        "outputId": "c92e41fd-25a5-4ac5-ecaf-40cacee24890"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The island was eggs in his most ary where a portion of this vessel nor\n",
            "me, true to admire\n",
            "ascending beyond of ears on each side operation, which contained the passengers which devoured them to be only forward and to speed again\n",
            "by Pencroft.\n",
            "\n",
            "“It is delia. In the perimeter of the past, was already passing from a new till threatelessul, but\n",
            "the colonists fell back, which\n",
            "was always received in any uneasy and calmer; but we\n",
            "will been recalleed, this evening there was not without green meal, friend was for th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "생성된 샘플의 예측 가능성을 조절하기 위해(즉, 생성된 텍스트가 훈련 텍스트에서 학습된 패턴을 따르게 할지 랜덤하게 생성할지 조절하기 위해) RNN이 계산한 로짓을 tf.random.categorical() 샘플링 함수로 전달하기 전에 스케일을 조정할 수 있다.\n",
        "\n",
        "스케일링 인자 a를 물리학의 온도의 역수로 해석할 수 있다. 온도가 높으면 무작위성이 커지고 온도가 낮으면 예측 가능한 행동을 만든다.\n",
        "\n",
        "a < 1로 로짓의 스케일을 조정하면 소프트맥스 함수가 계산할 확률은 더 균일해진다."
      ],
      "metadata": {
        "id": "6QznzPPisOJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = np.array([[1.0, 1.0, 3.0]])\n",
        "\n",
        "print('스케일 조정 전의 확률: ', tf.math.softmax(logits).numpy()[0])\n",
        "\n",
        "print('0.5배 조정 후 확률:  ', tf.math.softmax(0.5*logits).numpy()[0])\n",
        "\n",
        "print('0.1배 조정 후 확률:  ', tf.math.softmax(0.1*logits).numpy()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO6VRv_htTfq",
        "outputId": "94a2f282-e1fe-4886-ad3a-0d86cfed4f03"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "스케일 조정 전의 확률:  [0.10650698 0.10650698 0.78698604]\n",
            "0.5배 조정 후 확률:   [0.21194156 0.21194156 0.57611688]\n",
            "0.1배 조정 후 확률:   [0.31042377 0.31042377 0.37915245]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a = 0.1로 로짓의 스케일을 조정하면 거의 균등한 확률을 얻는다. "
      ],
      "metadata": {
        "id": "MvHcCJhLzF1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a=2.0 -> 예측 가능성이 높아짐\n",
        "tf.random.set_seed(1)\n",
        "print(sample(model, starting_str='The island', \n",
        "             scale_factor=2.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "232Gty26ziPM",
        "outputId": "e21cd545-ea79-42fa-8499-a282f5156859"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The island was extremely still trees, and a few months this very stream which they had once extremely profitical. The engineer was heard, and the colonists were allowed them to be surprised to the corral, and a few minutes he expected to the ocean, and as the reporter and Neb and Pencroft also accompanied the engineer, who was lost to be best to despair, since the lad was about to reply to the east. They had stailed on the\n",
            "surface of the earth in the corral.\n",
            "\n",
            "The colonists were obliged to reply to the int\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a=0.5 -> 무작위성이 높아짐\n",
        "tf.random.set_seed(1)\n",
        "print(sample(model, starting_str='The island', \n",
        "             scale_factor=0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM4OBvXVzjNk",
        "outputId": "864f4609-c81e-4e68-a393-a9969b110991"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The island had egbloes hollig’t grye?\n",
            "\n",
            "“He musmel!” cratcif-Cave band,\n",
            "my, froj wembles, “and thart?” he remaces. No, enohes aboup\n",
            "hig a;\n",
            "unforciDent, yourse head, was\n",
            "Towen under-siepplifin oyesior,’!\n",
            "\n",
            "“We can, at lew var.\n",
            "He\n",
            "\n",
            "Who is how pehocious delbay rock to estage on.\n",
            "Hadder\n",
            "Neb! capabarly, wized he\n",
            "would excet? Afre! we little inscotch\n",
            "Ayr?” ?Hisfermening, whno Aferailiza,” her masside yol dwombled! Unlivilly 3uspur\n",
            "her year.\n",
            "They extremity very day--were-fall, with the rooms, dlen’s sanch.\n",
            "\n",
            "AA9usi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a=0.5로 로짓의 스케일을 조정하면 더 랜덤한 텍스트가 생성된다. 올바른 텍스트와 신선한 텍스트 생성 사이에서 절충점을 찾아야 한다.\n",
        "\n",
        "시퀀스-투-시퀀스(sequence-to-sequence, seq2seq) 모델링 작업인 문자 수준의 텍스트 생성 문제를 다루었다. 이것 자체로는 아주 유용하지는 않지만 이런 종류의 모델에 맞는 애플리케이션이 있다. 예를 들어 비슷한 RNN 모델을 훈련하여 간단한 질문에 답변하는 챗봇을 사용할 수 있다."
      ],
      "metadata": {
        "id": "0REibSQ60I67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16.4 트랜스포머 모델을 사용한 언어 이해\n",
        "\n",
        "* 트랜스포머: 입력과 출력 시퀀스 사이에 있는 전역 의존성(global dependency)을 모델링할 수 있다.\n",
        "    * 트랜스포머 구조는 어텐션(attention)이라는 개념을 기반으로 한다.(셀프 어텐션 메커니즘)\n",
        "\n",
        "### 16.4.1 셀프 어텐션 메커니즘 이해\n",
        "\n",
        "#### 셀프 어텐션 기본 구조\n",
        "\n",
        "* 입력 시퀀스: 길이가 T인 $x^{(0)}, x^{(1)}, ... , x^{(T)}$\n",
        "* 출력 시퀀스: 길이가 T인 $O^{(0)}, O^{(1)}, ... , O^{(T)}$\n",
        "\n",
        "시퀀스 각 원소 $x^{(t)}와 O^{(t)}$는 크기가 d인 벡터이다. 그 다음 seq2seq 작업에서 셀프 어텐션은 입력 원소에 대한 출력 시퀀스에 있는 각 원소의 의존성을 모델링하는 것이 목적이다. 이를 위해 어텐션 메커니즘은 세 단계로 구성된다.\n",
        "\n",
        "1. 현재 원소와 시퀀스에 있는 다른 모든 원소 사이의 유사도를 기반으로 중요도 가중치를 계산\n",
        "2. 소프트맥스 함수를 사용하여 이 가중치를 정규화\n",
        "3. 이 가중치를 해당하는 시퀀스 원소와 결합하여 어텐션 값을 계산\n",
        "\n",
        "식으로 나타내면 셀프 어텐션의 출력은 모든 입력 시퀀스의 가중치 합이다. 예를 들어 i번째 입력 원소에 해당하는 출력은\n",
        "\n",
        "$$o^{(i)} = \\sum_{j=0}^{T}W_{ij}x^{j}$$\n",
        "\n",
        "여기서 가중치 $W_{ij}$는 현재 입력 원소 $x{i}$와 입력 시퀀스에 있는 다른 모든 원소 사이의 유사도를 기반으로 계산된다. 즉 이 유사도는 현재 입력 원소 $x^{(i)}$와 입력 시퀀스에 있는 다른 원소 $x^{(j)}$의 접곱으로 계산된다.\n",
        "\n",
        "$$w_{ij}=x^{(i)^T}x^{(j)}$$\n",
        "\n",
        "i번째 입력과 시퀀스에 있는 모든 입력($x^{(i)}$에서 $x^{(T)}$까지)에 대해 유사도 기반 가중치를 계산한 후 원본 가중치($w_{i0}$에서 $w_{iT}$까지)를 다음과 같이 소프트맥스 함수로 정규화한다.\n",
        "\n",
        "$$W_{ij} = \\frac{exp(w_{ij})}{\\sum_{j=0}^Texp(w_{ij})} = softmax([w_{ij}]_{j=0..T})$$\n",
        "\n",
        "소프트맥스 함수를 적용하기 때문에 정규화된 가중치 합은 1이 된다.\n",
        "\n",
        "$$\\sum^T_{j=0}W_{ij}=1$$\n",
        "\n",
        "정리하면 셀프 어텐션 연산의 주요 세 단계는 다음과 같다.\n",
        "\n",
        "1. 주어진 입력 원소 x^{(i)}와 [0, T] 범위에 있는 j번째 원소에 대해 점곱 $x^{(i)^T}x^{(j)}$를 계산\n",
        "2. 소프트맥스 함수로 이 점곱을 정규화하여 가중치 $W_{ij}$를 얻는다\n",
        "3. 전체 입력 시퀀스에 대한 가중치 합으로 출력 $O^{(i)}$를 계산\n",
        "\n"
      ],
      "metadata": {
        "id": "K27EVFku1o3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 셀프 어텐션 과정\n",
        "Image(url='https://git.io/JLdVo', width=700)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "f5c55llQ3Bpm",
        "outputId": "7e5041b0-c14a-40fb-ebad-4fe03501c2a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://git.io/JLdVo\" width=\"700\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 쿼리, 키, 값 가중치를 가진 셀프 어텐션 메카니즘\n",
        "\n",
        "언어 모델을 훈련할 때 분류 오차를 최소화하는 것 같이 목적 함수를 최적화하려면 입력 원소 $x^{(i)}$가 되는 단어 임베딩(즉, 입력 벡터)을 바꾸어야 한다. 다시 말해 기본적인 셀프 어텐션 메커니즘을 사용하면 트랜스포머 모델이 주어진 시퀀스에서 모델을 최적화하는 동안 어텐을 값을 바꾸거나 업데이트하는 데 제한적이다.\n",
        "\n",
        "셀프 어텐션 메커니즘을 모델 최적화에 대해 유연하고 적응할 수 있게 만들기 위해 추가적인 가중치 행렬을 사용한다. 이 가중치는 모델을 훈련하는 동안 학습되는 모델 파라미터이다. 이 세 가지 가중치를 $U_q, U_k, U_v$로 표시한다. 이 가중치는 입력을 쿼리, 키, 값 시퀀스로 만들기 위해 사용된다.\n",
        "\n",
        "* 쿼리 시퀀스: $q^{(i)}=U_qx^{(i)}$, $i \\in [0, T]$일 때\n",
        "* 키 시퀀스: $k^{(i)}=U_kx^{(i)}$, $i \\in [0, T]$일 때\n",
        "* 값 시퀀스: $v^{(i)}=U_vx^{(i)}$, $i \\in [0, T]$일 때\n",
        "\n",
        "여기서 $q^{(i)}$와 $k^{(i)}$는 크기가 $d_k$인 벡터이다. 따라서 투영된 행렬 $U_q$와 $U_k$의 크기는 $d_k * d$이다. 반면 $U_v$의 크기는 $d_v * d$이다. 예를 들어 이 벡터의 크기를 $m=d_k=d_v$로 동일할 때, 입력 시퀀스 원소 $x^{(i)}$와 j번째 시퀀스 원소 $x^{(j)}$ 사이에 접곱으로 정규화되지 않은 가중치를 계산하는 대신에 쿼리와 키 사이에 점곱을 계산한다.\n",
        "\n",
        "$$w_{ij}=q^{(i)^T}k^{(j)}$$\n",
        "\n",
        "그다음 소프트맥스 함수로 가중치 $w_{ij}$를 정규화하기 전에 $1/\\sqrt{m}$로 스케일을 조정한다.\n",
        "\n",
        "$$W_{ij}=softmax(\\frac{w_{ij}}{\\sqrt{m}})$$\n",
        "\n",
        "$1/\\sqrt{m}$로 $w_{ij}$의 스케일을 조정하면 가중치 벡터의 유클리드 길이가 거의 같은 범위에 있게 된다."
      ],
      "metadata": {
        "id": "4IKQAEs23CvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16.4.2 멀티-헤드 어텐션과 트랜스포머 블록\n",
        "\n",
        "셀프 어텐션 메커니즘의 판별 능력을 크게 높이는 또 다른 기술은 여러 개의 셀프 어텐션 연산을 합친 **멀티 헤드 어텐션**(Multi-Head Attention, MHA)이다. 이 경우 각 셀프 어텐션 메커니즘을 헤드(head)라고 부르며 병렬로 계산할 수 있다. r개의 병렬 헤드를 사용하여 각 헤드는 크기가 m인 벡터 h를 만든다. 이 벡터를 연결하여 크기가 r * m인 벡터 z를 얻는다. 마지막으로 이 연결된 벡터와 출력 행렬 $W^o$를 접곱하여 다음과 같이 최종 출력을 만든다.\n",
        "\n",
        "$$o^{(i)}=W_{ij}^oz$$"
      ],
      "metadata": {
        "id": "0_drGCWM3EFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 트랜스포머 블록 구조\n",
        "Image(url='https://git.io/JLdV6', width=700)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "sEg1T_UP3FGM",
        "outputId": "de487426-d330-45ee-f5b5-6e9f2e854464"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://git.io/JLdV6\" width=\"700\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 그림에 나온 트랜스포머 구조에는 언급하지 않은 두 가지 구성 요소가 추가되어 있다.\n",
        "\n",
        "이 중 하나는 **잔차 해결**(residual connection)이다. 잔차 연결은 층(또는 층 그룹)의 출력을 입력에 더한다. 즉, x + layer(x)이다. 잔차 연결로 층(또는 층 그룹)을 구성하는 블록을 **잔차 블록**(residual block)이라고 한다. 위 그림의 트랜스포머 블록은 두 개의 잔차 블록을 포함한다.\n",
        "\n",
        "다른 하나는 **층 정규화**(layer normalization)이다. 정규화 층의 한 종류이다. \n",
        "\n",
        "트랜스포머 모델은 먼저 입력 시퀀스가 셀프 어텐션 메커니즘을 기반으로 하는 MHA 층으로 전달된다. 또한, 입력 시퀀스가 잔차 연결을 통해 MHA 층의 출력에 더해진다. 이렇게 하면 훈련하는 동안 앞쪽의 층이 충분한 그레이디언트 신호를 받게 된다. 훈련 속도와 수렴을 향상시키기 위해 자주 사용하는 기법이다.\n",
        "\n",
        "입력 시퀀스가 MHA 층의 출력에 더해진 후 이 출력이 층 정규화를 통해 정규화된다. 정규화된 신호가 연속된 MLP(완전 연결) 층과 잔차 연결을 통과한다. 마지막으로 잔차 블록의 출력을 다시 정규화하여 출력 시퀀스로 반환해서 시퀀스 분류나 시퀀스 생성에 사용할 수 있다."
      ],
      "metadata": {
        "id": "YEoL060ChLAY"
      }
    }
  ]
}